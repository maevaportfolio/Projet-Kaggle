{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ce notebook combine :\n",
    "1. Le merge de test +les données meteo\n",
    "2. Le nettoyage et la préparation des données\n",
    "3. La création de features temporelles\n",
    "4. La sauvegarde des datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTIE 1 : MERGE DES DONNEES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Id    week  region_code region_name\n",
      "0  3235  201352           42      ALSACE\n",
      "1  3236  201352           72   AQUITAINE\n",
      "2  3237  201352           83    AUVERGNE\n",
      "(62, 5)\n",
      "['ID', 'Nom', 'Latitude', 'Longitude', 'Altitude']\n",
      "     ID              Nom   Latitude  Longitude  Altitude\n",
      "0  7005        ABBEVILLE  50.136000   1.834000        69\n",
      "1  7015    LILLE-LESQUIN  50.570000   3.097500        47\n",
      "2  7020  PTE DE LA HAGUE  49.725167  -1.939833         6\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv('../data/raw/test.csv')\n",
    "\n",
    "print(df_test.head(3))\n",
    "\n",
    "# Stations météo\n",
    "df_stations = pd.read_csv('../data/raw/ListedesStationsMeteo.csv', sep=';')\n",
    "print(df_stations.shape)\n",
    "print(df_stations.columns.tolist())\n",
    "print(df_stations.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping région / Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 associations station-région\n",
      "  Régions couvertes : 22\n",
      "  numer_sta      region_name\n",
      "0     07190           ALSACE\n",
      "1     07280           ALSACE\n",
      "2     07510        AQUITAINE\n",
      "3     07630        AQUITAINE\n",
      "4     07460         AUVERGNE\n",
      "5     07380         AUVERGNE\n",
      "6     07027  BASSE-NORMANDIE\n",
      "7     07139  BASSE-NORMANDIE\n",
      "8     07280        BOURGOGNE\n",
      "9     07255        BOURGOGNE\n"
     ]
    }
   ],
   "source": [
    "REGION_STATION_MAPPING = {\n",
    "    'ALSACE': ['07190', '07280'],\n",
    "    'AQUITAINE': ['07510', '07630'],\n",
    "    'AUVERGNE': ['07460', '07380'],\n",
    "    'BASSE-NORMANDIE': ['07027', '07139'],\n",
    "    'BOURGOGNE': ['07280', '07255'],\n",
    "    'BRETAGNE': ['07110', '07117', '07130'],\n",
    "    'CENTRE': ['07255', '07149'],\n",
    "    'CHAMPAGNE-ARDENNE': ['07072', '07168'],\n",
    "    'CORSE': ['07761', '07790'],\n",
    "    'FRANCHE-COMTE': ['07299', '07280'],\n",
    "    'HAUTE-NORMANDIE': ['07037', '07020'],\n",
    "    'ILE-DE-FRANCE': ['07150', '07149'],\n",
    "    'LANGUEDOC-ROUSSILLON': ['07630', '07643'],\n",
    "    'LIMOUSIN': ['07434', '07335'],\n",
    "    'LORRAINE': ['07090', '07180'],\n",
    "    'MIDI-PYRENEES': ['07630', '07627'],\n",
    "    'NORD-PAS-DE-CALAIS': ['07005', '07015'],\n",
    "    'PAYS DE LA LOIRE': ['07222', '07130'],\n",
    "    'PICARDIE': ['07005', '07015'],\n",
    "    'POITOU-CHARENTES': ['07335', '07255'],\n",
    "    \"PROVENCE-ALPES-COTE D'AZUR\": ['07650', '07690'],\n",
    "    'RHONE-ALPES': ['07481', '07482'],\n",
    "}\n",
    "\n",
    "station_region_map = []\n",
    "for region, stations in REGION_STATION_MAPPING.items():\n",
    "    for station in stations:\n",
    "        station_region_map.append({\n",
    "            'numer_sta': station,\n",
    "            'region_name': region\n",
    "        })\n",
    "\n",
    "df_station_region = pd.DataFrame(station_region_map)\n",
    "print (len(df_station_region), \"associations station-région\")\n",
    "print(f\"  Régions couvertes : {df_station_region['region_name'].nunique()}\")\n",
    "print(df_station_region.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÉTAPE 3 : Charger les données météo (SYNOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichiers dans le dossier :\n",
      "  - synop.200401.csv\n",
      "  - synop.200402.csv\n",
      "  - synop.200403.csv\n",
      "  - synop.200404.csv\n",
      "  - synop.200405.csv\n",
      "  - synop.200406.csv\n",
      "  - synop.200407.csv\n",
      "  - synop.200408.csv\n",
      "  - synop.200409.csv\n",
      "  - synop.200410.csv\n",
      "\n",
      "Fichiers synop trouvés : 24\n",
      "Chargement des données de 01/2012...\n",
      "Chargement des données de 02/2012...\n",
      "Chargement des données de 03/2012...\n",
      "Chargement des données de 04/2012...\n",
      "Chargement des données de 05/2012...\n",
      "Chargement des données de 06/2012...\n",
      "Chargement des données de 07/2012...\n",
      "Chargement des données de 08/2012...\n",
      "Chargement des données de 09/2012...\n",
      "Chargement des données de 10/2012...\n",
      "Chargement des données de 11/2012...\n",
      "Chargement des données de 12/2012...\n",
      "Chargement des données de 01/2013...\n",
      "Chargement des données de 02/2013...\n",
      "Chargement des données de 03/2013...\n",
      "Chargement des données de 04/2013...\n",
      "Chargement des données de 05/2013...\n",
      "Chargement des données de 06/2013...\n",
      "Chargement des données de 07/2013...\n",
      "Chargement des données de 08/2013...\n",
      "Chargement des données de 09/2013...\n",
      "Chargement des données de 10/2013...\n",
      "Chargement des données de 11/2013...\n",
      "Chargement des données de 12/2013...\n",
      "\n",
      "Données météo SYNOP : (339655, 60)\n",
      "  Colonnes : ['numer_sta', 'date', 'pmer', 'tend', 'cod_tend', 'dd', 'ff', 't', 'td', 'u', 'vv', 'ww', 'w1', 'w2', 'n', 'nbas', 'hbas', 'cl', 'cm', 'ch', 'pres', 'niv_bar', 'geop', 'tend24', 'tn12', 'tn24', 'tx12', 'tx24', 'tminsol', 'sw', 'tw', 'raf10', 'rafper', 'per', 'etat_sol', 'ht_neige', 'ssfrai', 'perssfrai', 'rr1', 'rr3', 'rr6', 'rr12', 'rr24', 'phenspe1', 'phenspe2', 'phenspe3', 'phenspe4', 'nnuage1', 'ctype1', 'hnuage1', 'nnuage2', 'ctype2', 'hnuage2', 'nnuage3', 'ctype3', 'hnuage3', 'nnuage4', 'ctype4', 'hnuage4', 'Unnamed: 59']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "chemin_dossier = r\"../data/raw/DonneesMeteorologiques\"\n",
    "\n",
    "print(\"Fichiers dans le dossier :\")\n",
    "tous_fichiers = os.listdir(chemin_dossier)\n",
    "for f in tous_fichiers[:10]:\n",
    "    print(f\"  - {f}\")\n",
    "\n",
    "# Chercher les fichiers synop pour 2012 et 2013 pour le test\n",
    "synop_files = sorted(glob.glob(os.path.join(chemin_dossier, \"synop.201[23]*.csv\")))\n",
    "print(f\"\\nFichiers synop trouvés : {len(synop_files)}\")\n",
    "\n",
    "if len(synop_files) > 0:\n",
    "    # Charger et concaténer\n",
    "    synop_dfs = []\n",
    "    for file in synop_files:\n",
    "        # Extraire l'année et le mois du nom de fichier\n",
    "        nom_fichier = os.path.basename(file).split('.')[1]  \n",
    "        annee = nom_fichier[:4]  # Les 4 premiers caractères = année\n",
    "        mois = nom_fichier[4:]   # Les 2 derniers = mois\n",
    "        print(f\"Chargement des données de {mois}/{annee}...\")\n",
    "        \n",
    "        df = pd.read_csv(file, sep=';')\n",
    "        synop_dfs.append(df)\n",
    "    \n",
    "    df_synop = pd.concat(synop_dfs, ignore_index=True)\n",
    "    print(f\"\\nDonnées météo SYNOP : {df_synop.shape}\")\n",
    "    print(f\"  Colonnes : {df_synop.columns.tolist()}\")\n",
    "else:\n",
    "    print(\"Aucun fichier synop trouvé \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "['../data/raw/DonneesMeteorologiques\\\\synop.201201.csv', '../data/raw/DonneesMeteorologiques\\\\synop.201202.csv', '../data/raw/DonneesMeteorologiques\\\\synop.201203.csv', '../data/raw/DonneesMeteorologiques\\\\synop.201204.csv', '../data/raw/DonneesMeteorologiques\\\\synop.201205.csv']\n"
     ]
    }
   ],
   "source": [
    "print(len(synop_files))\n",
    "print(synop_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données synop : (167760, 60)\n",
      "Stations uniques : 29\n",
      "Période : 2012-01-01 00:00:00 -> 2013-12-31 21:00:00\n"
     ]
    }
   ],
   "source": [
    "stations_of_interest = df_station_region['numer_sta'].unique().tolist()\n",
    "stations_int = [int(s) for s in stations_of_interest]\n",
    "\n",
    "synop_data_list = []\n",
    "for file in synop_files:\n",
    "    df_synop = pd.read_csv(file, sep=';', low_memory=False)\n",
    "    df_synop_filtered = df_synop[df_synop['numer_sta'].isin(stations_of_interest)]\n",
    "    \n",
    "    if len(df_synop_filtered) == 0:\n",
    "        df_synop_filtered = df_synop[df_synop['numer_sta'].isin(stations_int)]\n",
    "    \n",
    "    if len(df_synop_filtered) > 0:\n",
    "        synop_data_list.append(df_synop_filtered)\n",
    "\n",
    "df_synop_all = pd.concat(synop_data_list, ignore_index=True)\n",
    "df_synop_all['date'] = pd.to_datetime(df_synop_all['date'], format='%Y%m%d%H%M%S', errors='coerce')\n",
    "\n",
    "print(f\"Données synop : {df_synop_all.shape}\")\n",
    "print(f\"Stations uniques : {df_synop_all['numer_sta'].nunique()}\")\n",
    "print(f\"Période : {df_synop_all['date'].min()} -> {df_synop_all['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÉTAPE 4 : Gestion des dates (les convertir en week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201152, 201152, 201152, 201152, 201152]\n"
     ]
    }
   ],
   "source": [
    "df_synop_all['date'] = pd.to_datetime(df_synop_all['date'], format='%Y%m%d%H%M%S', errors='coerce')\n",
    "\n",
    "# Supprimer les lignes où la date n'a pas pu être convertie\n",
    "df_synop_all = df_synop_all.dropna(subset=['date'])\n",
    "\n",
    "df_synop_all['year'] = df_synop_all['date'].dt.isocalendar().year\n",
    "df_synop_all['week'] = df_synop_all['date'].dt.isocalendar().week\n",
    "df_synop_all['week_year'] = df_synop_all['year'].astype(str) + df_synop_all['week'].astype(str).str.zfill(2)\n",
    "df_synop_all['week_year'] = df_synop_all['week_year'].astype(int)\n",
    "\n",
    "print(df_synop_all['week_year'].head().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÉTAPE 5 : Agrégation par région et semaine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Variables météo disponibles : 31/31\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# S'assurer que les IDs sont au même format\n",
    "df_synop_all['numer_sta'] = df_synop_all['numer_sta'].astype(str).str.zfill(5)\n",
    "df_station_region['numer_sta'] = df_station_region['numer_sta'].astype(str).str.zfill(5)\n",
    "\n",
    "# Merger avec le mapping station->région\n",
    "df_synop_all = df_synop_all.merge(df_station_region, on='numer_sta', how='inner')\n",
    "\n",
    "\n",
    "# Variables météo à agréger\n",
    "meteo_vars = [\n",
    "    'tend', 'dd', 'ff', 't', 'td', 'u', 'vv', 'n', 'nbas', 'hbas',\n",
    "    'pres', 'niv_bar', 'geop', 'tend24', 'tn12', 'tn24', 'tx12', 'tx24',\n",
    "    'tminsol', 'tw', 'raf10', 'rafper', 'per', 'ht_neige', 'ssfrai',\n",
    "    'perssfrai', 'rr1', 'rr3', 'rr6', 'rr12', 'rr24'\n",
    "]\n",
    "\n",
    "meteo_vars_available = [v for v in meteo_vars if v in df_synop_all.columns]\n",
    "print(f\"  Variables météo disponibles : {len(meteo_vars_available)}/{len(meteo_vars)}\")\n",
    "\n",
    "# Convertir en numérique\n",
    "for var in meteo_vars_available:\n",
    "    df_synop_all[var] = pd.to_numeric(df_synop_all[var], errors='coerce')\n",
    "\n",
    "# Agréger par région et semaine (moyenne)\n",
    "agg_dict = {var: 'mean' for var in meteo_vars_available}\n",
    "df_meteo_agg = df_synop_all.groupby(['region_name', 'week_year'], as_index=False).agg(agg_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÉTAPE 6 : Merge final avec test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Merge effectué : (2288, 38)\n",
      "\n",
      "Fichier généré : test_synop_merged_inner.csv\n",
      "  Dimensions : (2288, 35)\n",
      "  Observations : 2288\n",
      "  Variables météo ajoutées : 31\n"
     ]
    }
   ],
   "source": [
    "# Normaliser les noms de régions\n",
    "df_test['region_name_clean'] = df_test['region_name'].str.upper().str.strip()\n",
    "df_meteo_agg['region_name_clean'] = df_meteo_agg['region_name'].str.upper().str.strip()\n",
    "# Merger\n",
    "df_final = df_test.merge(\n",
    "    df_meteo_agg,\n",
    "    left_on=['region_name_clean', 'week'],\n",
    "    right_on=['region_name_clean', 'week_year'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\" Merge effectué : {df_final.shape}\")\n",
    "\n",
    "# Nettoyer les colonnes dupliquées\n",
    "cols_to_drop = ['region_name_clean', 'week_year', 'region_name_y']\n",
    "df_final = df_final.drop(columns=[c for c in cols_to_drop if c in df_final.columns])\n",
    "\n",
    "if 'region_name_x' in df_final.columns:\n",
    "    df_final = df_final.rename(columns={'region_name_x': 'region_name'})\n",
    "\n",
    "# Sauvegarder\n",
    "df_final.to_csv('../data/processed/test_synop_merged_inner.csv', index=False)\n",
    "\n",
    "print(f\"\\nFichier généré : test_synop_merged_inner.csv\")\n",
    "print(f\"  Dimensions : {df_final.shape}\")\n",
    "print(f\"  Observations : {len(df_final)}\")\n",
    "print(f\"  Variables météo ajoutées : {len(meteo_vars_available)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse du merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  test.csv original : 2288 lignes\n",
      "  Après merge : 2288 lignes\n",
      "  Taux de couverture : 100.00%\n",
      "\n",
      "Régions couvertes :\n",
      "region_name\n",
      "ALSACE                        104\n",
      "AQUITAINE                     104\n",
      "AUVERGNE                      104\n",
      "BASSE-NORMANDIE               104\n",
      "BOURGOGNE                     104\n",
      "BRETAGNE                      104\n",
      "CENTRE                        104\n",
      "CHAMPAGNE-ARDENNE             104\n",
      "CORSE                         104\n",
      "FRANCHE-COMTE                 104\n",
      "HAUTE-NORMANDIE               104\n",
      "LANGUEDOC-ROUSSILLON          104\n",
      "LIMOUSIN                      104\n",
      "LORRAINE                      104\n",
      "MIDI-PYRENEES                 104\n",
      "NORD-PAS-DE-CALAIS            104\n",
      "PAYS-DE-LA-LOIRE              104\n",
      "PICARDIE                      104\n",
      "POITOU-CHARENTES              104\n",
      "PROVENCE-ALPES-COTE-D-AZUR    104\n",
      "ILE-DE-FRANCE                 104\n",
      "RHONE-ALPES                   104\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Période couverte :\n",
      "  Semaines : 201201 -> 201352\n",
      "\n",
      "Valeurs manquantes :\n",
      "niv_bar      2288\n",
      "tend24       2288\n",
      "geop         2288\n",
      "tx24         2288\n",
      "tw           2288\n",
      "tn24         2288\n",
      "raf10        1452\n",
      "ssfrai        417\n",
      "perssfrai     417\n",
      "nbas          413\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"  test.csv original : {len(df_test)} lignes\")\n",
    "print(f\"  Après merge : {len(df_final)} lignes\")\n",
    "print(f\"  Taux de couverture : {len(df_final)/len(df_test)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nRégions couvertes :\")\n",
    "print(df_final['region_name'].value_counts())\n",
    "\n",
    "print(f\"\\nPériode couverte :\")\n",
    "print(f\"  Semaines : {df_final['week'].min()} -> {df_final['week'].max()}\")\n",
    "\n",
    "print(f\"\\nValeurs manquantes :\")\n",
    "missing = df_final.isnull().sum()\n",
    "missing = missing[missing > 0].sort_values(ascending=False)\n",
    "if len(missing) > 0:\n",
    "    print(missing.head(10))\n",
    "else:\n",
    "    print(\"  Aucune valeur manquante!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [missing_count, missing_pct]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = pd.DataFrame({\n",
    "    'missing_count': df.isnull().sum(),\n",
    "    'missing_pct': df.isnull().mean() * 100\n",
    "}).query(\"missing_count > 0\").sort_values('missing_pct', ascending=False)\n",
    "\n",
    "missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PARTIE 2 : NETTOYAGE ET PRÉPARATION DES DONNÉES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger le fichier mergé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2288, 35)\n",
      "['Id', 'week', 'region_code', 'region_name', 'tend', 'dd', 'ff', 't', 'td', 'u', 'vv', 'n', 'nbas', 'hbas', 'pres', 'niv_bar', 'geop', 'tend24', 'tn12', 'tn24', 'tx12', 'tx24', 'tminsol', 'tw', 'raf10', 'rafper', 'per', 'ht_neige', 'ssfrai', 'perssfrai', 'rr1', 'rr3', 'rr6', 'rr12', 'rr24']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2288 entries, 0 to 2287\n",
      "Data columns (total 35 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   Id           2288 non-null   int64  \n",
      " 1   week         2288 non-null   int64  \n",
      " 2   region_code  2288 non-null   int64  \n",
      " 3   region_name  2288 non-null   object \n",
      " 4   tend         1976 non-null   float64\n",
      " 5   dd           1976 non-null   float64\n",
      " 6   ff           1976 non-null   float64\n",
      " 7   t            1976 non-null   float64\n",
      " 8   td           1976 non-null   float64\n",
      " 9   u            1976 non-null   float64\n",
      " 10  vv           1976 non-null   float64\n",
      " 11  n            1976 non-null   float64\n",
      " 12  nbas         1875 non-null   float64\n",
      " 13  hbas         1976 non-null   float64\n",
      " 14  pres         1976 non-null   float64\n",
      " 15  niv_bar      0 non-null      float64\n",
      " 16  geop         0 non-null      float64\n",
      " 17  tend24       0 non-null      float64\n",
      " 18  tn12         1976 non-null   float64\n",
      " 19  tn24         0 non-null      float64\n",
      " 20  tx12         1976 non-null   float64\n",
      " 21  tx24         0 non-null      float64\n",
      " 22  tminsol      1976 non-null   float64\n",
      " 23  tw           0 non-null      float64\n",
      " 24  raf10        836 non-null    float64\n",
      " 25  rafper       1976 non-null   float64\n",
      " 26  per          1976 non-null   float64\n",
      " 27  ht_neige     1976 non-null   float64\n",
      " 28  ssfrai       1871 non-null   float64\n",
      " 29  perssfrai    1871 non-null   float64\n",
      " 30  rr1          1976 non-null   float64\n",
      " 31  rr3          1976 non-null   float64\n",
      " 32  rr6          1976 non-null   float64\n",
      " 33  rr12         1976 non-null   float64\n",
      " 34  rr24         1976 non-null   float64\n",
      "dtypes: float64(31), int64(3), object(1)\n",
      "memory usage: 625.8+ KB\n",
      "None\n",
      "(2288, 35)\n",
      "['Id', 'week', 'region_code', 'region_name', 'tend', 'dd', 'ff', 't', 'td', 'u', 'vv', 'n', 'nbas', 'hbas', 'pres', 'niv_bar', 'geop', 'tend24', 'tn12', 'tn24', 'tx12', 'tx24', 'tminsol', 'tw', 'raf10', 'rafper', 'per', 'ht_neige', 'ssfrai', 'perssfrai', 'rr1', 'rr3', 'rr6', 'rr12', 'rr24']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('test_synop_merged_inner.csv')\n",
    "print(df.shape)\n",
    "print(df.columns.tolist())\n",
    "\n",
    "print(df.info())\n",
    "\n",
    "print(df.shape)\n",
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÉTAPE 1 : Supprimer les colonnes redondantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    }
   ],
   "source": [
    "# Chercher les fichiers synop\n",
    "synop_files = sorted(glob.glob(os.path.join(chemin_dossier, \"synop.*.csv\")))\n",
    "\n",
    "print(len(synop_files))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÉTAPE 2 : Analyser et supprimer les colonnes avec >30% de NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Colonnes avec valeurs manquantes :\n",
      "              Column  Missing_Count  Missing_Percent\n",
      "niv_bar      niv_bar           2288           100.00\n",
      "tend24        tend24           2288           100.00\n",
      "geop            geop           2288           100.00\n",
      "tx24            tx24           2288           100.00\n",
      "tw                tw           2288           100.00\n",
      "tn24            tn24           2288           100.00\n",
      "raf10          raf10           1452            63.46\n",
      "ssfrai        ssfrai            417            18.23\n",
      "perssfrai  perssfrai            417            18.23\n",
      "nbas            nbas            413            18.05\n",
      "u                  u            312            13.64\n",
      "vv                vv            312            13.64\n",
      "tend            tend            312            13.64\n",
      "dd                dd            312            13.64\n",
      "ff                ff            312            13.64\n",
      "tn12            tn12            312            13.64\n",
      "n                  n            312            13.64\n",
      "hbas            hbas            312            13.64\n",
      "pres            pres            312            13.64\n",
      "t                  t            312            13.64\n",
      "td                td            312            13.64\n",
      "tminsol      tminsol            312            13.64\n",
      "tx12            tx12            312            13.64\n",
      "per              per            312            13.64\n",
      "rafper        rafper            312            13.64\n",
      "ht_neige    ht_neige            312            13.64\n",
      "rr1              rr1            312            13.64\n",
      "rr3              rr3            312            13.64\n",
      "rr6              rr6            312            13.64\n",
      "rr12            rr12            312            13.64\n",
      "rr24            rr24            312            13.64\n",
      "\n",
      "Colonnes à supprimer (>30% NaN) : 7\n",
      "  ['niv_bar', 'tend24', 'geop', 'tx24', 'tw', 'tn24', 'raf10']\n",
      "\n",
      "  Avant : 35 colonnes\n",
      "  Après : 28 colonnes\n",
      "\n",
      "Colonnes restantes (28) :\n",
      "  ['Id', 'week', 'region_code', 'region_name', 'tend', 'dd', 'ff', 't', 'td', 'u', 'vv', 'n', 'nbas', 'hbas', 'pres', 'tn12', 'tx12', 'tminsol', 'rafper', 'per', 'ht_neige', 'ssfrai', 'perssfrai', 'rr1', 'rr3', 'rr6', 'rr12', 'rr24']\n"
     ]
    }
   ],
   "source": [
    "missing_data = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percent': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "\n",
    "missing_data = missing_data[missing_data['Missing_Count'] > 0].sort_values('Missing_Percent', ascending=False)\n",
    "\n",
    "print(f\"\\nColonnes avec valeurs manquantes :\")\n",
    "print(missing_data)\n",
    "\n",
    "cols_to_drop = missing_data[missing_data['Missing_Percent'] > 30]['Column'].tolist()\n",
    "print(f\"\\nColonnes à supprimer (>30% NaN) : {len(cols_to_drop)}\")\n",
    "print(f\"  {cols_to_drop}\")\n",
    "\n",
    "df_clean = df.drop(columns=cols_to_drop)\n",
    "print(f\"\\n  Avant : {df.shape[1]} colonnes\")\n",
    "print(f\"  Après : {df_clean.shape[1]} colonnes\")\n",
    "print(f\"\\nColonnes restantes ({df_clean.shape[1]}) :\")\n",
    "print(f\"  {df_clean.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÉTAPE 3 : Conversion des types et création de colonnes temporelles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>week</th>\n",
       "      <th>region_code</th>\n",
       "      <th>region_name</th>\n",
       "      <th>tend</th>\n",
       "      <th>dd</th>\n",
       "      <th>ff</th>\n",
       "      <th>t</th>\n",
       "      <th>td</th>\n",
       "      <th>u</th>\n",
       "      <th>...</th>\n",
       "      <th>rafper</th>\n",
       "      <th>per</th>\n",
       "      <th>ht_neige</th>\n",
       "      <th>ssfrai</th>\n",
       "      <th>perssfrai</th>\n",
       "      <th>rr1</th>\n",
       "      <th>rr3</th>\n",
       "      <th>rr6</th>\n",
       "      <th>rr12</th>\n",
       "      <th>rr24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3235</td>\n",
       "      <td>201352</td>\n",
       "      <td>42</td>\n",
       "      <td>ALSACE</td>\n",
       "      <td>8.035714</td>\n",
       "      <td>186.250000</td>\n",
       "      <td>4.346429</td>\n",
       "      <td>280.184821</td>\n",
       "      <td>277.112500</td>\n",
       "      <td>81.901786</td>\n",
       "      <td>...</td>\n",
       "      <td>8.157143</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.245536</td>\n",
       "      <td>0.664286</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>2.646429</td>\n",
       "      <td>5.257143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3236</td>\n",
       "      <td>201352</td>\n",
       "      <td>72</td>\n",
       "      <td>AQUITAINE</td>\n",
       "      <td>3.303571</td>\n",
       "      <td>201.607143</td>\n",
       "      <td>4.844643</td>\n",
       "      <td>282.049107</td>\n",
       "      <td>278.773214</td>\n",
       "      <td>80.696429</td>\n",
       "      <td>...</td>\n",
       "      <td>8.841071</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-47.195122</td>\n",
       "      <td>0.219643</td>\n",
       "      <td>0.596429</td>\n",
       "      <td>1.171429</td>\n",
       "      <td>2.542857</td>\n",
       "      <td>4.564286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3237</td>\n",
       "      <td>201352</td>\n",
       "      <td>83</td>\n",
       "      <td>AUVERGNE</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>202.321429</td>\n",
       "      <td>5.732143</td>\n",
       "      <td>281.217857</td>\n",
       "      <td>275.412500</td>\n",
       "      <td>67.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.528571</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-45.555556</td>\n",
       "      <td>0.021429</td>\n",
       "      <td>0.078571</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>0.657143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3238</td>\n",
       "      <td>201352</td>\n",
       "      <td>25</td>\n",
       "      <td>BASSE-NORMANDIE</td>\n",
       "      <td>7.117117</td>\n",
       "      <td>215.765766</td>\n",
       "      <td>5.974775</td>\n",
       "      <td>280.480631</td>\n",
       "      <td>278.551802</td>\n",
       "      <td>87.882883</td>\n",
       "      <td>...</td>\n",
       "      <td>10.328829</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-45.000000</td>\n",
       "      <td>0.368182</td>\n",
       "      <td>1.014679</td>\n",
       "      <td>2.385185</td>\n",
       "      <td>4.007407</td>\n",
       "      <td>8.553846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3239</td>\n",
       "      <td>201352</td>\n",
       "      <td>26</td>\n",
       "      <td>BOURGOGNE</td>\n",
       "      <td>5.714286</td>\n",
       "      <td>198.839286</td>\n",
       "      <td>5.208929</td>\n",
       "      <td>280.675893</td>\n",
       "      <td>277.626786</td>\n",
       "      <td>81.839286</td>\n",
       "      <td>...</td>\n",
       "      <td>9.166964</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-45.652174</td>\n",
       "      <td>0.180357</td>\n",
       "      <td>0.545536</td>\n",
       "      <td>0.864286</td>\n",
       "      <td>2.242857</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id    week  region_code      region_name      tend          dd        ff  \\\n",
       "0  3235  201352           42           ALSACE  8.035714  186.250000  4.346429   \n",
       "1  3236  201352           72        AQUITAINE  3.303571  201.607143  4.844643   \n",
       "2  3237  201352           83         AUVERGNE  2.857143  202.321429  5.732143   \n",
       "3  3238  201352           25  BASSE-NORMANDIE  7.117117  215.765766  5.974775   \n",
       "4  3239  201352           26        BOURGOGNE  5.714286  198.839286  5.208929   \n",
       "\n",
       "            t          td          u  ...     rafper   per  ht_neige  ssfrai  \\\n",
       "0  280.184821  277.112500  81.901786  ...   8.157143 -10.0       0.0     NaN   \n",
       "1  282.049107  278.773214  80.696429  ...   8.841071 -10.0       0.0     0.0   \n",
       "2  281.217857  275.412500  67.500000  ...  10.528571 -10.0       0.0     0.0   \n",
       "3  280.480631  278.551802  87.882883  ...  10.328829 -10.0       0.0     0.0   \n",
       "4  280.675893  277.626786  81.839286  ...   9.166964 -10.0       0.0     0.0   \n",
       "\n",
       "   perssfrai       rr1       rr3       rr6      rr12      rr24  \n",
       "0        NaN  0.245536  0.664286  1.125000  2.646429  5.257143  \n",
       "1 -47.195122  0.219643  0.596429  1.171429  2.542857  4.564286  \n",
       "2 -45.555556  0.021429  0.078571  0.114286  0.371429  0.657143  \n",
       "3 -45.000000  0.368182  1.014679  2.385185  4.007407  8.553846  \n",
       "4 -45.652174  0.180357  0.545536  0.864286  2.242857  4.500000  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Colonne 'date' créée\n",
      "  Période : 2012-01-02 00:00:00 → 2013-12-23 00:00:00\n",
      "  Total : 721 jours\n",
      "\n",
      " Types de colonnes :\n",
      "date           datetime64[ns]\n",
      "week                    int64\n",
      "region_code             int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "def week_to_datetime(week_str):\n",
    "    \"\"\"Convertit 'AAAASS' en première date de la semaine\"\"\"\n",
    "    year = int(str(week_str)[:4])\n",
    "    week = int(str(week_str)[4:6])\n",
    "    jan4 = datetime(year, 1, 4)\n",
    "    week_one_monday = jan4 - pd.Timedelta(days=jan4.weekday())\n",
    "    week_start = week_one_monday + pd.Timedelta(weeks=week-1)\n",
    "    return week_start\n",
    "\n",
    "df_clean['date'] = df_clean['week'].apply(week_to_datetime)\n",
    "df_clean['date'] = pd.to_datetime(df_clean['date'])\n",
    "\n",
    "print(f\" Colonne 'date' créée\")\n",
    "print(f\"  Période : {df_clean['date'].min()} → {df_clean['date'].max()}\")\n",
    "print(f\"  Total : {(df_clean['date'].max() - df_clean['date'].min()).days} jours\")\n",
    "\n",
    "print(f\"\\n Types de colonnes :\")\n",
    "print(df_clean[['date', 'week', 'region_code']].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÉTAPE 4 : Imputation des valeurs manquantes par la médiane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes avant imputation :\n",
      "ssfrai       417\n",
      "perssfrai    417\n",
      "nbas         413\n",
      "tend         312\n",
      "td           312\n",
      "dd           312\n",
      "ff           312\n",
      "t            312\n",
      "n            312\n",
      "vv           312\n",
      "u            312\n",
      "hbas         312\n",
      "tx12         312\n",
      "tminsol      312\n",
      "pres         312\n",
      "tn12         312\n",
      "per          312\n",
      "rafper       312\n",
      "ht_neige     312\n",
      "rr1          312\n",
      "rr3          312\n",
      "rr6          312\n",
      "rr12         312\n",
      "rr24         312\n",
      "dtype: int64\n",
      "\n",
      "Imputation par médiane (colonnes numériques)...\n",
      "   tend : 0.85\n",
      "   dd : 193.21\n",
      "   ff : 3.34\n",
      "   t : 284.86\n",
      "   td : 280.46\n",
      "   u : 76.88\n",
      "   vv : 22941.96\n",
      "   n : 77.08\n",
      "   nbas : 4.47\n",
      "   hbas : 1404.95\n",
      "   pres : 100070.44\n",
      "   tn12 : 281.34\n",
      "   tx12 : 288.05\n",
      "   tminsol : 279.29\n",
      "   rafper : 6.16\n",
      "   per : -10.00\n",
      "   ht_neige : 0.00\n",
      "   ssfrai : 0.00\n",
      "   perssfrai : -45.00\n",
      "   rr1 : 0.06\n",
      "   rr3 : 0.19\n",
      "   rr6 : 0.33\n",
      "   rr12 : 0.76\n",
      "   rr24 : 1.52\n",
      "\n",
      "Aucune valeur manquante restante : True\n",
      "\n",
      "Aperçu final du dataset :\n",
      "               Id           week  region_code         tend           dd  \\\n",
      "count  2288.00000    2288.000000  2288.000000  2288.000000  2288.000000   \n",
      "mean   4378.50000  201276.500000    51.363636     0.385211   190.332540   \n",
      "min    3235.00000  201201.000000    11.000000   -53.750000    41.428571   \n",
      "25%    3806.75000  201226.750000    25.000000    -7.874839   171.053571   \n",
      "50%    4378.50000  201276.500000    47.500000     0.851834   193.214286   \n",
      "75%    4950.25000  201326.250000    74.000000     8.322273   213.482143   \n",
      "max    5522.00000  201352.000000    94.000000    55.089286   321.785714   \n",
      "std     660.63303      52.215339    26.517449    16.134330    39.065148   \n",
      "\n",
      "                ff            t           td            u            vv  ...  \\\n",
      "count  2288.000000  2288.000000  2288.000000  2288.000000   2288.000000  ...   \n",
      "mean      3.510771   284.925549   280.388072    76.211103  23702.295042  ...   \n",
      "min       0.932143   264.516964   259.293750    46.812500   2478.181818  ...   \n",
      "25%       2.860045   280.837277   277.188182    71.407265  18169.642857  ...   \n",
      "50%       3.340516   284.862054   280.463839    76.875000  22941.964286  ...   \n",
      "75%       3.955580   289.365751   284.441855    82.178571  28369.791667  ...   \n",
      "max       9.380357   299.603571   292.297321    96.854545  58771.929825  ...   \n",
      "std       1.031470     6.108117     5.115820     8.296273   8566.999615  ...   \n",
      "\n",
      "          per     ht_neige       ssfrai    perssfrai          rr1  \\\n",
      "count  2288.0  2288.000000  2288.000000  2288.000000  2288.000000   \n",
      "mean    -10.0     0.000891     0.000147   -44.919033     0.084162   \n",
      "min     -10.0     0.000000     0.000000   -60.000000     0.000000   \n",
      "25%     -10.0     0.000000     0.000000   -45.189873     0.021429   \n",
      "50%     -10.0     0.000000     0.000000   -45.000000     0.057400   \n",
      "75%     -10.0     0.000000     0.000000   -44.444444     0.117857   \n",
      "max     -10.0     0.062353     0.040000   -41.739130     0.642857   \n",
      "std       0.0     0.005230     0.001200     1.019066     0.088484   \n",
      "\n",
      "               rr3          rr6         rr12         rr24                 date  \n",
      "count  2288.000000  2288.000000  2288.000000  2288.000000                 2288  \n",
      "mean      0.254810     0.489627     1.019100     2.049069  2012-12-27 12:00:00  \n",
      "min       0.000000     0.000000     0.000000     0.000000  2012-01-02 00:00:00  \n",
      "25%       0.073214     0.107143     0.300000     0.571429  2012-06-30 06:00:00  \n",
      "50%       0.189087     0.334524     0.757143     1.521429  2012-12-27 12:00:00  \n",
      "75%       0.355609     0.650000     1.394643     2.844643  2013-06-25 18:00:00  \n",
      "max       2.040000     5.616667     8.600000    15.957143  2013-12-23 00:00:00  \n",
      "std       0.251345     0.544989     0.999522     2.018158                  NaN  \n",
      "\n",
      "[8 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "remaining_missing = df_clean.isnull().sum()\n",
    "remaining_missing = remaining_missing[remaining_missing > 0].sort_values(ascending=False)\n",
    "print(\"Valeurs manquantes avant imputation :\")\n",
    "print(remaining_missing)\n",
    "\n",
    "print(f\"\\nImputation par médiane (colonnes numériques)...\")\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if df_clean[col].isnull().sum() > 0:\n",
    "        median_val = df_clean[col].median()\n",
    "        df_clean[col].fillna(median_val, inplace=True)\n",
    "        print(f\"   {col} : {median_val:.2f}\")\n",
    "\n",
    "print(f\"\\nAucune valeur manquante restante : {df_clean.isnull().sum().sum() == 0}\")\n",
    "print(f\"\\nAperçu final du dataset :\")\n",
    "print(df_clean.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÉTAPE 5 : Vérifier la fréquence des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Du : 2012-01-02 00:00:00\n",
      "   Au : 2013-12-23 00:00:00\n",
      "   Total : 721 jours\n",
      "\n",
      "Distribution par région :\n",
      "count     22.0\n",
      "mean     104.0\n",
      "std        0.0\n",
      "min      104.0\n",
      "25%      104.0\n",
      "50%      104.0\n",
      "75%      104.0\n",
      "max      104.0\n",
      "Name: count, dtype: float64\n",
      "\n",
      "Vérification des doublons (region_code + week) :\n",
      "   Doublons trouvés : 0\n",
      "\n",
      "Semaines disponibles : [np.int64(201201), np.int64(201202), np.int64(201203), np.int64(201204), np.int64(201205), np.int64(201206), np.int64(201207), np.int64(201208), np.int64(201209), np.int64(201210)]... (104 semaines)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"   Du : {df_clean['date'].min()}\")\n",
    "print(f\"   Au : {df_clean['date'].max()}\")\n",
    "print(f\"   Total : {(df_clean['date'].max() - df_clean['date'].min()).days} jours\")\n",
    "\n",
    "print(\"\\nDistribution par région :\")\n",
    "region_counts = df_clean['region_code'].value_counts().sort_index()\n",
    "print(region_counts.describe())\n",
    "\n",
    "print(f\"\\nVérification des doublons (region_code + week) :\")\n",
    "duplicates = df_clean.duplicated(subset=['region_code', 'week'], keep=False)\n",
    "print(f\"   Doublons trouvés : {duplicates.sum()}\")\n",
    "\n",
    "print(f\"\\nSemaines disponibles : {sorted(df_clean['week'].unique())[:10]}... ({len(df_clean['week'].unique())} semaines)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÉTAPE 6 : Créer la colonne saison\n",
    "Créer une variable saison (Hiver/Printemps/Été/Automne) pour capturer la saisonnalité de la grippe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Répartition par saison :\n",
      "saison\n",
      "Hiver        572\n",
      "Automne      572\n",
      "Ete          572\n",
      "Printemps    572\n",
      "Name: count, dtype: int64\n",
      "        date saison\n",
      "0 2013-12-23  Hiver\n",
      "1 2013-12-23  Hiver\n",
      "2 2013-12-23  Hiver\n",
      "3 2013-12-23  Hiver\n",
      "4 2013-12-23  Hiver\n",
      "5 2013-12-23  Hiver\n",
      "6 2013-12-23  Hiver\n",
      "7 2013-12-23  Hiver\n",
      "8 2013-12-23  Hiver\n",
      "9 2013-12-23  Hiver\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_season(date):\n",
    "    \"\"\"Détermine la saison en fonction du mois\"\"\"\n",
    "    month = date.month\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Hiver'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Printemps'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Ete'\n",
    "    else:\n",
    "        return 'Automne'\n",
    "\n",
    "df_clean['saison'] = df_clean['date'].apply(get_season)\n",
    "\n",
    "print(\"Répartition par saison :\")\n",
    "print(df_clean['saison'].value_counts())\n",
    "\n",
    "print(df_clean[['date', 'saison']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÉTAPE 7 : Créer le dataset avec variables sélectionnées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variables à conserver (groupe réduit) :\n",
      "  - tminsol : température min du sol\n",
      "  - nbas : nébulosité des nuages niveau inférieur\n",
      "  - n : nébulosité totale\n",
      "  - td : point de rosée\n",
      "  - ff : vitesse du vent\n",
      "  - t : température\n",
      "  - u : humidité\n",
      "  - vv : visibilité horizontale\n",
      "\n",
      "Variables trouvées : 8/8\n",
      "\n",
      "Dataset complet : (2288, 30)\n",
      "Dataset réduit (variables sélectionnées) : (2288, 13)\n",
      "Colonnes du dataset réduit : ['region_code', 'region_name', 'week', 'date', 'saison', 'tminsol', 'nbas', 'n', 'td', 'ff', 't', 'u', 'vv']\n"
     ]
    }
   ],
   "source": [
    "# variables sélectionnées basées sur l'analyse de corrélation\n",
    "variables_selectionnees = ['tminsol', 'nbas', 'n', 'td', 'ff', 't', 'u', 'vv']\n",
    "\n",
    "print(f\"\\nVariables à conserver (groupe réduit) :\")\n",
    "print(f\"  - tminsol : température min du sol\")\n",
    "print(f\"  - nbas : nébulosité des nuages niveau inférieur\")\n",
    "print(f\"  - n : nébulosité totale\")\n",
    "print(f\"  - td : point de rosée\")\n",
    "print(f\"  - ff : vitesse du vent\")\n",
    "print(f\"  - t : température\")\n",
    "print(f\"  - u : humidité\")\n",
    "print(f\"  - vv : visibilité horizontale\")\n",
    "\n",
    "# vérifier que toutes les variables existent\n",
    "variables_disponibles = [var for var in variables_selectionnees if var in df_clean.columns]\n",
    "variables_manquantes = [var for var in variables_selectionnees if var not in df_clean.columns]\n",
    "\n",
    "print(f\"\\nVariables trouvées : {len(variables_disponibles)}/{len(variables_selectionnees)}\")\n",
    "if variables_manquantes:\n",
    "    print(f\"Variables non trouvées : {variables_manquantes}\")\n",
    "\n",
    "# créer df_reduit avec uniquement les variables sélectionnées\n",
    "colonnes_obligatoires = [ 'region_code', 'region_name', 'week', 'date', 'saison']\n",
    "df_reduit = df_clean[colonnes_obligatoires + variables_disponibles].copy()\n",
    "\n",
    "print(f\"\\nDataset complet : {df_clean.shape}\")\n",
    "print(f\"Dataset réduit (variables sélectionnées) : {df_reduit.shape}\")\n",
    "print(f\"Colonnes du dataset réduit : {df_reduit.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÉTAPE 8 : Créer des features temporelles (lags et moyennes mobiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAUVEGARDE DES DATASETS FINAUX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Dataset complet (toutes variables) :\n",
      "   Fichier : test_meteo_full.csv\n",
      "   Shape : (2288, 30)\n",
      "   Colonnes : 30\n",
      "\n",
      "2. Dataset réduit (variables sélectionnées) :\n",
      "   Fichier : test_meteo_reduit.csv\n",
      "   Shape : (2288, 13)\n",
      "   Colonnes : ['region_code', 'region_name', 'week', 'date', 'saison', 'tminsol', 'nbas', 'n', 'td', 'ff', 't', 'u', 'vv']\n"
     ]
    }
   ],
   "source": [
    "#  Dataset complet (toutes les variables)\n",
    "df_clean.to_csv('../data/processed/test_meteo_full.csv', index=False)\n",
    "print(\"\\n1. Dataset complet (toutes variables) :\")\n",
    "print(f\"   Fichier : test_meteo_full.csv\")\n",
    "print(f\"   Shape : {df_clean.shape}\")\n",
    "print(f\"   Colonnes : {len(df_clean.columns)}\")\n",
    "\n",
    "#  Dataset réduit (variables sélectionnées)\n",
    "df_reduit.to_csv('../data/processed/test_meteo_reduit.csv', index=False)\n",
    "print(\"\\n2. Dataset réduit (variables sélectionnées) :\")\n",
    "print(f\"   Fichier : test_meteo_reduit.csv\")\n",
    "print(f\"   Shape : {df_reduit.shape}\")\n",
    "print(f\"   Colonnes : {df_reduit.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RÉSUMÉ FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  test_meteo_full.csv : 2288 lignes x 30 colonnes\n",
      "  test_meteo_reduit.csv : 2288 lignes x 13 colonnes\n",
      "\n",
      "Variables sélectionnées (8) :\n",
      "  - tminsol\n",
      "  - nbas\n",
      "  - n\n",
      "  - td\n",
      "  - ff\n",
      "  - t\n",
      "  - u\n",
      "  - vv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"  test_meteo_full.csv : {df_clean.shape[0]} lignes x {df_clean.shape[1]} colonnes\")\n",
    "print(f\"  test_meteo_reduit.csv : {df_reduit.shape[0]} lignes x {df_reduit.shape[1]} colonnes\")\n",
    "\n",
    "print(f\"\\nVariables sélectionnées ({len(variables_disponibles)}) :\")\n",
    "for var in variables_disponibles:\n",
    "    print(f\"  - {var}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flu-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
